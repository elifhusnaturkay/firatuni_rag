import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Model yÃ¼kle (GPU varsa kullanÄ±lÄ±r)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "ytu-ce-cosmos/turkish-gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# AynÄ± embedding modeli
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Chunk'larÄ± yÃ¼kle
with open("chunks.txt", "r", encoding="utf-8") as f:
    chunks = f.read().split("\n\n---\n\n")

# FAISS indexâ€™i yÃ¼kle
index = faiss.read_index("vector_index.index")

# KullanÄ±cÄ±dan soru al
query = input("Soru: ")
query_embedding = embedding_model.encode(query).astype("float32")

# En yakÄ±n 5 chunkâ€™Ä± bul
D, I = index.search(np.array([query_embedding]), k=5)
relevant_chunks = [chunks[i] for i in I[0]]
context = "\n".join(relevant_chunks)

# Prompta baÄŸlamla birlikte soru ver
prompt = f"""
AÅŸaÄŸÄ±daki bilgiye gÃ¶re soruyu yanÄ±tla:

{context}

Soru: {query}
Cevap:
"""

# Tokenize ve modele ver
inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

# CevabÄ± yazdÄ±r
print("\nğŸ¤– Cevap:\n")
print(answer.replace(prompt.strip(), "").strip())
